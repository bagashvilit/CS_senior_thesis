{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "copy_only",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The model only has copying mechanism. Train and validate it on respective datasets.\n",
        "\n",
        "The model will be saved in model_output directory in the colab session. Make sure to donload the weights, vocabulary, and config info, because after the session end they will be lost\n",
        "\n",
        "The model is evaluated on test set and the predicions will be saved in predict_out directory. Make sure to save them as well\n",
        "\n",
        "The BLEU score with current parameters on valid and test set is 0.19 approximately. Which is almost same as Corec ugmented model results.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ixln1IXv8DcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive to Colab"
      ],
      "metadata": {
        "id": "2DjWRqU6L6oP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFS-a5Dwtd4W",
        "outputId": "f7916a4a-2d49-40ec-e284-d0e865aa47f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required dependencies"
      ],
      "metadata": {
        "id": "kFANUBxKMAK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp-models\n",
        "!pip install --upgrade google-cloud-storage\n",
        "!!pip install allennlp"
      ],
      "metadata": {
        "id": "kb_EeqnLtmzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory in runtime files where the trained model will be saved. (PLease note that once the runtime is disconected the files will be lost)"
      ],
      "metadata": {
        "id": "pDqhonAmME1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model_output"
      ],
      "metadata": {
        "id": "N8qVQHUbWO9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model hyperparameters and dataset reader and train the model\n",
        "Training on GPU takes about 1h and 34min\n",
        "\n",
        "Paths to the training and testing datasets is specified:\n",
        "\"train_data_path\": \"/content/drive/My Drive/combined_data/train.tsv\",\n",
        "\"validation_data_path\": \"/content/drive/My Drive/combined_data/valid.tsv\",\n",
        "Please provide your own path instead"
      ],
      "metadata": {
        "id": "ud-MA_HUMcCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp_models.generation.dataset_readers.copynet_seq2seq import CopyNetDatasetReader\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
        "import tempfile\n",
        "import json\n",
        "from allennlp_models.generation.dataset_readers import copynet_seq2seq\n",
        "from allennlp.commands.train import train_model\n",
        "from allennlp.common.params import Params\n",
        "\n",
        "\n",
        "config = {\n",
        "  \"dataset_reader\": {\n",
        "    \"source_tokenizer\": {\n",
        "        \"split_on_spaces\":True,\n",
        "        },\n",
        "    \"target_namespace\": \"target_tokens\",\n",
        "    \"type\": \"copynet_seq2seq\",\n",
        "    \"source_token_indexers\": {\n",
        "      \"tokens\": {\n",
        "        \"type\": \"single_id\",\n",
        "        \"namespace\": \"source_tokens\"\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"vocabulary\": {\n",
        "  \"min_count\": {\n",
        "    \"source_tokens\": 5,\n",
        "    \"target_tokens\": 5,   \n",
        "  },\n",
        "  \"oov_token\":'',\n",
        "},\n",
        "\"train_data_path\": \"/content/drive/My Drive/combined_data/train.tsv\",\n",
        "\"validation_data_path\": \"/content/drive/My Drive/combined_data/valid.tsv\",\n",
        "\"model\": {\n",
        "  \"type\": \"copynet_seq2seq\",\n",
        "  \"source_embedder\": {\n",
        "      \"token_embedders\": {\"tokens\": {\"type\": \"embedding\", \"embedding_dim\": 512, \"trainable\": True, \"vocab_namespace\": \"source_tokens\"},               \n",
        "      },\n",
        "\n",
        "  },\n",
        "  \"encoder\": {\n",
        "    \"type\": \"lstm\",\n",
        "    \"input_size\": 512,\n",
        "    \"hidden_size\": 512,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bidirectional\": True\n",
        "  },\n",
        "  \"attention\": {\n",
        "    \"type\": \"bilinear\",\n",
        "    \"vector_dim\": 1024,\n",
        "    \"matrix_dim\": 1024,\n",
        "  },\n",
        "  # \"target_embedding_dim\": 100,\n",
        "  \"beam_size\": 5,\n",
        "  \"max_decoding_steps\": 50,\n",
        "  #'scheduled_sampling_ratio':0.5,\n",
        "},\n",
        "\"data_loader\": {\n",
        "\n",
        "  \"batch_size\" : 32,\n",
        "  \"shuffle\":True,\n",
        "},\n",
        "\"trainer\": {\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"adam\",\n",
        "    \"lr\": 0.001\n",
        "  },\n",
        "  \"learning_rate_scheduler\": {\n",
        "    \"type\": \"cosine\",\n",
        "    \"t_initial\": 5,\n",
        "    \"eta_mul\": 0.9\n",
        "  },\n",
        "  \"checkpointer\": {\n",
        "      \"keep_most_recent_by_count\": 17,\n",
        "    },\n",
        "  \"num_epochs\": 17,\n",
        "  \"cuda_device\": 0,\n",
        "  \"should_log_learning_rate\": True,\n",
        "  \"should_log_parameter_statistics\": False\n",
        "}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "serialization_dir = \"model_output\"\n",
        "config_filename = serialization_dir + \"/training_config.json\"\n",
        "with open(config_filename, \"w\") as config_file:\n",
        "    json.dump(config, config_file)\n",
        "\n",
        "\n",
        "model = train_model(\n",
        "    Params(config), serialization_dir, file_friendly_logging=True, force=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Wr4HeSeStowq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the saved pre-trained model weigths to create a new model and test on the test dataset"
      ],
      "metadata": {
        "id": "nfO2zuhGNE51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.models.model import Model\n",
        "from allennlp.common.params import Params\n",
        "from allennlp_models.generation.dataset_readers.copynet_seq2seq import CopyNetDatasetReader\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
        "import tempfile\n",
        "import json\n",
        "from allennlp_models.generation.dataset_readers import copynet_seq2seq\n",
        "\n",
        "# it's necessary to specify the configuration again\n",
        "config = {\n",
        "  \"dataset_reader\": {\n",
        "    \"source_tokenizer\": {\n",
        "        \"split_on_spaces\":True,\n",
        "        },\n",
        "    \"target_namespace\": \"target_tokens\",\n",
        "    \"type\": \"copynet_seq2seq\",\n",
        "    \"source_token_indexers\": {\n",
        "      \"tokens\": {\n",
        "        \"type\": \"single_id\",\n",
        "        \"namespace\": \"source_tokens\"\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"vocabulary\": {\n",
        "  \"min_count\": {\n",
        "    \"source_tokens\": 5,\n",
        "    \"target_tokens\": 5,   \n",
        "  },\n",
        "  \"oov_token\":'',\n",
        "},\n",
        "\"model\": {\n",
        "  \"type\": \"copynet_seq2seq\",\n",
        "  \"source_embedder\": {\n",
        "      \"token_embedders\": {\"tokens\": {\"type\": \"embedding\", \"embedding_dim\": 512, \"trainable\": True, \"vocab_namespace\": \"source_tokens\"},               \n",
        "      },\n",
        "\n",
        "  },\n",
        "  \"encoder\": {\n",
        "    \"type\": \"lstm\",\n",
        "    \"input_size\": 512,\n",
        "    \"hidden_size\": 512,\n",
        "    \"num_layers\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"bidirectional\": True\n",
        "  },\n",
        "  \"attention\": {\n",
        "    \"type\": \"bilinear\",\n",
        "    \"vector_dim\": 1024,\n",
        "    \"matrix_dim\": 1024,\n",
        "  },\n",
        "  # \"target_embedding_dim\": 100,\n",
        "  \"beam_size\": 5,\n",
        "  \"max_decoding_steps\": 50,\n",
        "  #'scheduled_sampling_ratio':0.5,\n",
        "},\n",
        "\"data_loader\": {\n",
        "\n",
        "  \"batch_size\" : 32,\n",
        "  \"shuffle\":True,\n",
        "},\n",
        "\"trainer\": {\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"adam\",\n",
        "    \"lr\": 0.001\n",
        "  },\n",
        "  \"learning_rate_scheduler\": {\n",
        "    \"type\": \"cosine\",\n",
        "    \"t_initial\": 5,\n",
        "    \"eta_mul\": 0.9\n",
        "  },\n",
        "  \"checkpointer\": {\n",
        "      \"keep_most_recent_by_count\": 17,\n",
        "    },\n",
        "  \"num_epochs\": 17,\n",
        "  \"cuda_device\": 0,\n",
        "  \"should_log_learning_rate\": True,\n",
        "  \"should_log_parameter_statistics\": False\n",
        "}\n",
        "}\n",
        "\n",
        "model = Model.load(Params(config),serialization_dir='model_output', weights_file=\"model_output/model_state_e14_b0.th\")"
      ],
      "metadata": {
        "id": "crqM61uXXIgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir predict_out"
      ],
      "metadata": {
        "id": "opxW4mdcwwdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictor\n",
        "from allennlp.data.dataset_readers import dataset_reader\n",
        "from allennlp_models.generation.dataset_readers.copynet_seq2seq import CopyNetDatasetReader\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.data_loaders.simple_data_loader import SimpleDataLoader\n",
        "from allennlp.training.util import evaluate\n",
        "from allennlp.data import DatasetReader\n",
        "\n",
        "reader = CopyNetDatasetReader(\n",
        "    target_namespace=\"target_tokens\",\n",
        "    source_token_indexers={'tokens': SingleIdTokenIndexer(namespace=\"source_tokens\")},\n",
        "    )\n",
        "\n",
        "from allennlp.predictors import Predictor\n",
        "\n",
        "predictor = Predictor(model.to('cuda'), reader)\n",
        "\n",
        "inputs = {\n",
        "    \"sentence\": \"mmm a / CHANGELOG . md <nl> ppp b / CHANGELOG . md <nl> # Changelog <nl> - # 2 . 2 . 0 ( 16 / 07 / 2015 ) - SNAPSHOT <nl> + # 2 . 1 . 1 ( 29 / 02 / 2016 ) - SNAPSHOT <nl> - Added AppCompat Styles\"\n",
        "}\n",
        "\n",
        "from allennlp.data import Instance\n",
        "\n",
        "print(Instance({\"sentence\": \"this is me\"}))\n",
        "\n",
        "predictor.predict_instance(Instance({\"sentence\": \"this is me\"}))"
      ],
      "metadata": {
        "id": "h8cqkr-5cVrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.data.dataset_readers import dataset_reader\n",
        "from allennlp_models.generation.dataset_readers.copynet_seq2seq import CopyNetDatasetReader\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "from allennlp.data.data_loaders.simple_data_loader import SimpleDataLoader\n",
        "from allennlp.training.util import evaluate\n",
        "from allennlp.data import DatasetReader\n",
        "\n",
        "reader = CopyNetDatasetReader(\n",
        "    target_namespace=\"target_tokens\",\n",
        "    source_token_indexers={'tokens': SingleIdTokenIndexer(namespace=\"source_tokens\")},\n",
        "    )\n",
        "\n",
        "test_data = list(reader.read(\"/content/drive/My Drive/combined_data/test_example.tsv\"))\n",
        "data_loader = SimpleDataLoader(test_data, batch_size=1)\n",
        "data_loader.index_with(model.vocab)"
      ],
      "metadata": {
        "id": "O3TS2AX1EGOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "results = evaluate(model, data_loader)\n",
        "#results = evaluate(model, data_loader, predictions_output_file='predict_out/predictions.tsv')\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(results)"
      ],
      "metadata": {
        "id": "F6X84FKBOz1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{'BLEU': 0.18798859277510044, 'loss': 30.710134963917582}"
      ],
      "metadata": {
        "id": "XVuZRGDfE84Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}